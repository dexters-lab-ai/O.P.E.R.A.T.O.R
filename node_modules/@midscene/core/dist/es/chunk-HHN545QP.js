import {
  ANTHROPIC_API_KEY,
  AZURE_OPENAI_API_VERSION,
  AZURE_OPENAI_DEPLOYMENT,
  AZURE_OPENAI_ENDPOINT,
  AZURE_OPENAI_KEY,
  MATCH_BY_POSITION,
  MIDSCENE_API_TYPE,
  MIDSCENE_AZURE_OPENAI_INIT_CONFIG_JSON,
  MIDSCENE_AZURE_OPENAI_SCOPE,
  MIDSCENE_DEBUG_AI_PROFILE,
  MIDSCENE_DEBUG_AI_RESPONSE,
  MIDSCENE_LANGSMITH_DEBUG,
  MIDSCENE_MODEL_NAME,
  MIDSCENE_OPENAI_INIT_CONFIG_JSON,
  MIDSCENE_OPENAI_SOCKS_PROXY,
  MIDSCENE_USE_ANTHROPIC_SDK,
  MIDSCENE_USE_AZURE_OPENAI,
  MIDSCENE_USE_QWEN_VL,
  MIDSCENE_USE_VLM_UI_TARS,
  OPENAI_API_KEY,
  OPENAI_BASE_URL,
  OPENAI_MAX_TOKENS,
  OPENAI_USE_AZURE,
  getAIConfig,
  getAIConfigInBoolean,
  getAIConfigInJson
} from "./chunk-DWFHUOJE.js";

// src/ai-model/service-caller/index.ts
import { Anthropic } from "@anthropic-ai/sdk";
import {
  DefaultAzureCredential,
  getBearerTokenProvider
} from "@azure/identity";
import { assert as assert3, enableDebug, getDebug } from "@midscene/shared/utils";
import { ifInBrowser } from "@midscene/shared/utils";
import dJSON from "dirty-json";
import OpenAI, { AzureOpenAI } from "openai";
import { SocksProxyAgent } from "socks-proxy-agent";

// src/ai-model/common.ts
import { assert } from "@midscene/shared/utils";
async function callAiFn(msgs, AIActionTypeValue) {
  assert(
    checkAIConfig(),
    "Cannot find config for AI model service. If you are using a self-hosted model without validating the API key, please set `OPENAI_API_KEY` to any non-null value. https://midscenejs.com/model-provider.html"
  );
  const { content, usage } = await callToGetJSONObject(
    msgs,
    AIActionTypeValue
  );
  return { content, usage };
}
function fillLocateParam(locate) {
  if (locate?.bbox_2d && !locate?.bbox) {
    locate.bbox = locate.bbox_2d;
    delete locate.bbox_2d;
  }
  const defaultBboxSize = 10;
  if (locate?.bbox) {
    locate.bbox[0] = Math.round(locate.bbox[0]);
    locate.bbox[1] = Math.round(locate.bbox[1]);
    locate.bbox[2] = typeof locate.bbox[2] === "number" ? Math.round(locate.bbox[2]) : Math.round(locate.bbox[0] + defaultBboxSize);
    locate.bbox[3] = typeof locate.bbox[3] === "number" ? Math.round(locate.bbox[3]) : Math.round(locate.bbox[1] + defaultBboxSize);
  }
  return locate;
}
var warned = false;
function warnGPT4oSizeLimit(size) {
  if (warned)
    return;
  if (getModelName()?.toLowerCase().includes("gpt-4o")) {
    const warningMsg = `GPT-4o has a maximum image input size of 2000x768 or 768x2000, but got ${size.width}x${size.height}. Please set your page to a smaller resolution. Otherwise, the result may be inaccurate.`;
    if (Math.max(size.width, size.height) > 2e3 || Math.min(size.width, size.height) > 768) {
      console.warn(warningMsg);
      warned = true;
    }
  }
}

// src/ai-model/prompt/ui-tars-planning.ts
function getTimeZoneInfo() {
  const timeZone = Intl.DateTimeFormat().resolvedOptions().timeZone;
  const offset = -(/* @__PURE__ */ new Date()).getTimezoneOffset() / 60;
  return {
    timezone: `UTC${offset >= 0 ? "+" : ""}${offset}`,
    isChina: timeZone === "Asia/Shanghai"
  };
}
var language = getTimeZoneInfo().isChina ? "Chinese" : "English";
var uiTarsPlanningPrompt = `
You are a GUI agent. You are given a task and your action history, with screenshots. You need to perform the next action to complete the task. 

## Output Format
\`\`\`
Thought: ...
Action: ...
\`\`\`

## Action Space
click(start_box='[x1, y1, x2, y2]')
left_double(start_box='[x1, y1, x2, y2]')
right_single(start_box='[x1, y1, x2, y2]')
drag(start_box='[x1, y1, x2, y2]', end_box='[x3, y3, x4, y4]')
hotkey(key='')
type(content='') #If you want to submit your input, use "\\n" at the end of \`content\`.
scroll(start_box='[x1, y1, x2, y2]', direction='down or up or right or left')
wait() #Sleep for 5s and take a screenshot to check for any changes.
finished()
call_user() # Submit the task and call the user when the task is unsolvable, or when you need the user's help.

## Note
- Use ${language} in \`Thought\` part.
- Write a small plan and finally summarize your next action (with its target element) in one sentence in \`Thought\` part.

## User Instruction
`;
var getSummary = (prediction) => prediction.replace(/Reflection:[\s\S]*?(?=Action_Summary:|Action:|$)/g, "").trim();

// src/ai-model/prompt/assertion.ts
var language2 = getTimeZoneInfo().isChina ? "Chinese" : "English";
var defaultAssertionPrompt = "You are a senior testing engineer. User will give an assertion and a screenshot of a page. By carefully viewing the screenshot, please tell whether the assertion is truthy.";
var defaultAssertionResponseJsonFormat = `Return in the following JSON format:
{
  pass: boolean, // whether the assertion is truthy
  thought: string | null, // string, if the result is falsy, give the reason why it is falsy. Otherwise, put null.
}`;
var uiTarsAssertionResponseJsonFormat = `## Output Json String Format
\`\`\`
"{
  "pass": <<is a boolean value from the enum [true, false], true means the assertion is truthy>>, 
  "thought": "<<is a string, give the reason why the assertion is falsy or truthy. Otherwise.>>"
}"
\`\`\`

## Rules **MUST** follow
- Make sure to return **only** the JSON, with **no additional** text or explanations.
- Use ${language2} in \`thought\` part.
- You **MUST** strictly follow up the **Output Json String Format**.`;
function systemPromptToAssert(model) {
  return `${defaultAssertionPrompt}

${model.isUITars ? uiTarsAssertionResponseJsonFormat : defaultAssertionResponseJsonFormat}`;
}
var assertSchema = {
  type: "json_schema",
  json_schema: {
    name: "assert",
    strict: true,
    schema: {
      type: "object",
      properties: {
        pass: {
          type: "boolean",
          description: "Whether the assertion passed or failed"
        },
        thought: {
          type: ["string", "null"],
          description: "The thought process behind the assertion"
        }
      },
      required: ["pass", "thought"],
      additionalProperties: false
    }
  }
};

// src/ai-model/prompt/llm-locator.ts
import { PromptTemplate } from "@langchain/core/prompts";
function systemPromptToLocateElement() {
  if (getAIConfigInBoolean(MATCH_BY_POSITION)) {
    return `
## Role:
You are an expert in software testing.

## Objective:
- Identify elements in screenshots and text that match the user's description.
- Give the coordinates of the element that matches the user's description best in the screenshot.

## Output Format:
\`\`\`json
{
  "bbox": [number, number, number, number], 
  "errors"?: string[]
}
\`\`\`

Fields:
* \`bbox\` is the bounding box of the element that matches the user's description best in the screenshot
* \`errors\` is an optional array of error messages (if any)
`;
  }
  return `
## Role:
You are an expert in software page image (2D) and page element text analysis.

## Objective:
- Identify elements in screenshots and text that match the user's description.
- Return JSON data containing the selection reason and element ID.

## Skills:
- Image analysis and recognition
- Multilingual text understanding
- Software UI design and testing

## Workflow:
1. Receive the user's element description, screenshot, and element description information. Note that the text may contain non-English characters (e.g., Chinese), indicating that the application may be non-English.
2. Based on the user's description, locate the target element ID in the list of element descriptions and the screenshot.
3. Found the required number of elements
4. Return JSON data containing the selection reason and element ID.

## Constraints:
- Strictly adhere to the specified location when describing the required element; do not select elements from other locations.
- Elements in the image with NodeType other than "TEXT Node" have been highlighted to identify the element among multiple non-text elements.
- Accurately identify element information based on the user's description and return the corresponding element ID from the element description information, not extracted from the image.
- If no elements are found, the "elements" array should be empty.
- The returned data must conform to the specified JSON format.
- The returned value id information must use the id from element info (important: **use id not indexId, id is hash content**)

## Output Format:

Please return the result in JSON format as follows:

\`\`\`json
{
  "elements": [
    // If no matching elements are found, return an empty array []
    {
      "reason": "PLACEHOLDER", // The thought process for finding the element, replace PLACEHOLDER with your thought process
      "text": "PLACEHOLDER", // Replace PLACEHOLDER with the text of elementInfo, if none, leave empty
      "id": "PLACEHOLDER" // Replace PLACEHOLDER with the ID (important: **use id not indexId, id is hash content**) of elementInfo
    }
    // More elements...
  ],
  "errors": [] // Array of strings containing any error messages
}
\`\`\`

## Example:
Example 1:
Input Example:
\`\`\`json
// Description: "Shopping cart icon in the upper right corner"
{
  "description": "PLACEHOLDER", // Description of the target element
  "screenshot": "path/screenshot.png",
  "text": '{
      "pageSize": {
        "width": 400, // Width of the page
        "height": 905 // Height of the page
      },
      "elementInfos": [
        {
          "id": "1231", // ID of the element
          "indexId": "0", // Index of the element，The image is labeled to the left of the element
          "attributes": { // Attributes of the element
            "nodeType": "IMG Node", // Type of element, types include: TEXT Node, IMG Node, BUTTON Node, INPUT Node
            "src": "https://ap-southeast-3.m",
            "class": ".img"
          },
          "content": "", // Text content of the element
          "rect": {
            "left": 280, // Distance from the left side of the page
            "top": 8, // Distance from the top of the page
            "width": 44, // Width of the element
            "height": 44 // Height of the element
          }
        },
        {
          "id": "66551", // ID of the element
          "indexId": "1", // Index of the element,The image is labeled to the left of the element
          "attributes": { // Attributes of the element
            "nodeType": "IMG Node", // Type of element, types include: TEXT Node, IMG Node, BUTTON Node, INPUT Node
            "src": "data:image/png;base64,iVBORw0KGgoAAAANSU...",
            "class": ".icon"
          },
          "content": "", // Text content of the element
          "rect": {
            "left": 350, // Distance from the left side of the page
            "top": 16, // Distance from the top of the page
            "width": 25, // Width of the element
            "height": 25 // Height of the element
          }
        },
        ...
        {
          "id": "12344",
          "indexId": "2", // Index of the element，The image is labeled to the left of the element
          "attributes": {
            "nodeType": "TEXT Node",
            "class": ".product-name"
          },
          "center": [
            288,
            834
          ],
          "content": "Mango Drink",
          "rect": {
            "left": 188,
            "top": 827,
            "width": 199,
            "height": 13
          }
        },
        ...
      ]
    }
  '
}
\`\`\`
Output Example:
\`\`\`json
{
  "elements": [
    {
      // Describe the reason for finding this element, replace with actual value in practice
      "reason": "Reason for finding element 4: It is located in the upper right corner, is an image type, and according to the screenshot, it is a shopping cart icon button",
      "text": "",
      // ID(**use id not indexId**) of this element, replace with actual value in practice, **use id not indexId**
      "id": "1231"
    }
  ],
  "errors": []
}
\`\`\`
  
  `;
}
var locatorSchema = {
  type: "json_schema",
  json_schema: {
    name: "find_elements",
    strict: true,
    schema: {
      type: "object",
      properties: {
        elements: {
          type: "array",
          items: {
            type: "object",
            properties: {
              reason: {
                type: "string",
                description: "Reason for finding this element"
              },
              text: {
                type: "string",
                description: "Text content of the element"
              },
              id: {
                type: "string",
                description: "ID of this element"
              }
            },
            required: ["reason", "text", "id"],
            additionalProperties: false
          },
          description: "List of found elements"
        },
        errors: {
          type: "array",
          items: {
            type: "string"
          },
          description: "List of error messages, if any"
        }
      },
      required: ["elements", "errors"],
      additionalProperties: false
    }
  }
};
var findElementPrompt = new PromptTemplate({
  template: `
Here is the item user want to find:
=====================================
{targetElementDescription}
=====================================

{pageDescription}
  `,
  inputVariables: ["pageDescription", "targetElementDescription"]
});

// src/ai-model/prompt/llm-planning.ts
import { PromptTemplate as PromptTemplate2 } from "@langchain/core/prompts";

// src/image/index.ts
import {
  imageInfo,
  imageInfoOfBase64,
  base64Encoded,
  resizeImg,
  transformImgPathToBase64,
  saveBase64Image,
  zoomForGPT4o
} from "@midscene/shared/img";

// src/ai-model/prompt/util.ts
import { NodeType } from "@midscene/shared/constants";
import { descriptionOfTree, treeToList } from "@midscene/shared/extractor";
import { assert as assert2 } from "@midscene/shared/utils";
import { generateHashId } from "@midscene/shared/utils";
function describeSize(size) {
  return `${size.width} x ${size.height}`;
}
function elementByPositionWithElementInfo(treeRoot, position) {
  assert2(typeof position !== "undefined", "position is required for query");
  const matchingElements = [];
  function dfs(node) {
    if (node?.node) {
      const item = node.node;
      if (item.rect.left <= position.x && position.x <= item.rect.left + item.rect.width && item.rect.top <= position.y && position.y <= item.rect.top + item.rect.height) {
        matchingElements.push(item);
      }
    }
    for (const child of node.children) {
      dfs(child);
    }
  }
  dfs(treeRoot);
  if (matchingElements.length === 0) {
    return void 0;
  }
  const element = matchingElements.reduce((smallest, current) => {
    const smallestArea = smallest.rect.width * smallest.rect.height;
    const currentArea = current.rect.width * current.rect.height;
    return currentArea < smallestArea ? current : smallest;
  });
  const distanceToCenter = distance(
    { x: element.center[0], y: element.center[1] },
    position
  );
  return distanceToCenter <= distanceThreshold ? element : void 0;
}
var distanceThreshold = 16;
function distance(point1, point2) {
  return Math.sqrt((point1.x - point2.x) ** 2 + (point1.y - point2.y) ** 2);
}
var samplePageDescription = `
And the page is described as follows:
====================
The size of the page: 1280 x 720
Some of the elements are marked with a rectangle in the screenshot corresponding to the markerId, some are not.

Description of all the elements in screenshot:
<div id="969f1637" markerId="1" left="100" top="100" width="100" height="100"> // The markerId indicated by the rectangle label in the screenshot
  <h4 id="b211ecb2" markerId="5" left="150" top="150" width="90" height="60">
    The username is accepted
  </h4>
  ...many more
</div>
====================
`;
async function describeUserPage(context, opt) {
  const { screenshotBase64 } = context;
  let width;
  let height;
  if (context.size) {
    ({ width, height } = context.size);
  } else {
    const imgSize = await imageInfoOfBase64(screenshotBase64);
    ({ width, height } = imgSize);
  }
  const treeRoot = context.tree;
  const idElementMap = {};
  const flatElements = treeToList(treeRoot);
  flatElements.forEach((element) => {
    idElementMap[element.id] = element;
    if (typeof element.indexId !== "undefined") {
      idElementMap[`${element.indexId}`] = element;
    }
  });
  const contentTree = await descriptionOfTree(
    treeRoot,
    opt?.truncateTextLength,
    opt?.filterNonTextContent
  );
  const pageJSONDescription = getAIConfigInBoolean(MATCH_BY_POSITION) ? "" : `Some of the elements are marked with a rectangle in the screenshot, some are not. 
 The page elements tree:
${contentTree}`;
  const sizeDescription = describeSize({ width, height });
  return {
    description: `The size of the page: ${sizeDescription} 
 ${pageJSONDescription}`,
    elementById(id) {
      assert2(typeof id !== "undefined", "id is required for query");
      const item = idElementMap[`${id}`];
      return item;
    },
    elementByPosition(position, size) {
      return elementByPositionWithElementInfo(treeRoot, position);
    },
    insertElementByPosition(position) {
      const rect = {
        left: Math.max(position.x - 4, 0),
        top: Math.max(position.y - 4, 0),
        width: 8,
        height: 8
      };
      const id = generateHashId(rect);
      const element = {
        id,
        attributes: { nodeType: NodeType.POSITION },
        rect,
        content: "",
        center: [position.x, position.y]
      };
      treeRoot.children.push({
        node: element,
        children: []
      });
      flatElements.push(element);
      idElementMap[id] = element;
      return element;
    },
    size: { width, height }
  };
}

// src/ai-model/prompt/llm-planning.ts
var qwenCoTLog = `"what_the_user_wants_to_do_next_by_instruction": string, // What the user wants to do according to the instruction and previous logs. `;
var qwenCurrentLog = `"log": string, // Log what the next one action (ONLY ONE!) you can do according to the screenshot and the instruction. The typical log looks like "I will use action {{ action-type }} to do ..". If no action should be done, log the reason. ". Use the same language as the user's instruction.`;
var llmCurrentLog = `"log": string, // Log what the next actions you can do according to the screenshot and the instruction. The typical log looks like "I will use action {{ action-type }} to do ..". If no action should be done, log the reason. ". Use the same language as the user's instruction.`;
var commonOutputFields = `"error"?: string, // Error messages about unexpected situations, if any. Only think it is an error when the situation is not expected according to the instruction. Use the same language as the user's instruction.
  "more_actions_needed_by_instruction": boolean, // Consider if there is still more action(s) to do after the action in "Log" is done, according to the instruction. If so, set this field to true. Otherwise, set it to false.`;
var qwenLocateParam = "locate: {bbox_2d: [number, number, number, number], prompt: string }";
var systemTemplateOfQwen = `
Target: User will give you a screenshot, an instruction and some previous logs indicating what have been done. Please tell what the next one action is (or null if no action should be done) to do the tasks the instruction requires. 

Restriction:
- Don't give extra actions or plans beyond the instruction. ONLY plan for what the instruction requires. For example, don't try to submit the form if the instruction is only to fill something.
- Always give ONLY ONE action in \`log\` field (or null if no action should be done), instead of multiple actions. Supported actions are Tap, Hover, Input, KeyboardPress, Scroll.
- Don't repeat actions in the previous logs.

Supporting actions:
- Tap: { type: "Tap", ${qwenLocateParam} }
- Hover: { type: "Hover", ${qwenLocateParam} }
- Input: { type: "Input", ${qwenLocateParam}, param: { value: string } } // \`value\` is the final that should be filled in the input box. No matter what modifications are required, just provide the final value to replace the existing input value. 
- KeyboardPress: { type: "KeyboardPress", param: { value: string } }
- Scroll: { type: "Scroll", ${qwenLocateParam} | null, param: { direction: 'down'(default) | 'up' | 'right' | 'left', scrollType: 'once' (default) | 'untilBottom' | 'untilTop' | 'untilRight' | 'untilLeft', distance: null | number }} // locate is the element to scroll. If it's a page scroll, put \`null\` in the \`locate\` field.

Field description:
* The \`prompt\` field inside the \`locate\` field is a short description that could be used to locate the element.

Return in JSON format:
{
  ${qwenCoTLog}
  ${qwenCurrentLog}
  ${commonOutputFields}
  "action": 
    {
      // one of the supporting actions
    } | null,
  ,
  "sleep"?: number, // The sleep time after the action, in milliseconds.
}
`;
var llmLocateParam = `locate: {{"id": string, "prompt": string}} | null`;
var systemTemplateOfLLM = `
## Role

You are a versatile professional in software UI automation. Your outstanding contributions will impact the user experience of billions of users.

## Objective

- Decompose the instruction user asked into a series of actions
- Locate the target element if possible
- If the instruction cannot be accomplished, give a further plan.

## Workflow

1. Receive the screenshot, element description of screenshot(if any), user's instruction and previous logs.
2. Decompose the user's task into a sequence of actions, and place it in the \`actions\` field. There are different types of actions (Tap / Hover / Input / KeyboardPress / Scroll / FalsyConditionStatement / Sleep). The "About the action" section below will give you more details.
3. Precisely locate the target element if it's already shown in the screenshot, put the location info in the \`locate\` field of the action.
4. If some target elements is not shown in the screenshot, consider the user's instruction is not feasible on this page. Follow the next steps.
5. Consider whether the user's instruction will be accomplished after all the actions
 - If yes, set \`taskWillBeAccomplished\` to true
 - If no, don't plan more actions by closing the array. Get ready to reevaluate the task. Some talent people like you will handle this. Give him a clear description of what have been done and what to do next. Put your new plan in the \`furtherPlan\` field. The "How to compose the \`taskWillBeAccomplished\` and \`furtherPlan\` fields" section will give you more details.

## Constraints

- All the actions you composed MUST be based on the page context information you get.
- Trust the "What have been done" field about the task (if any), don't repeat actions in it.
- Respond only with valid JSON. Do not write an introduction or summary or markdown prefix like \`\`\`json\`\`\`.
- If the screenshot and the instruction are totally irrelevant, set reason in the \`error\` field.

## About the \`actions\` field

The \`locate\` param is commonly used in the \`param\` field of the action, means to locate the target element to perform the action, it conforms to the following scheme:

type LocateParam = {{
  "id": string, // the id of the element found. It should either be the id marked with a rectangle in the screenshot or the id described in the description.
  "prompt"?: string // the description of the element to find. It can only be omitted when locate is null.
}} | null // If it's not on the page, the LocateParam should be null

## Supported actions

Each action has a \`type\` and corresponding \`param\`. To be detailed:
- type: 'Tap'
  * {{ ${llmLocateParam} }}
- type: 'Hover'
  * {{ ${llmLocateParam} }}
- type: 'Input', replace the value in the input field
  * {{ ${llmLocateParam}, param: {{ value: string }} }}
  * \`value\` is the final value that should be filled in the input field. No matter what modifications are required, just provide the final value user should see after the action is done. 
- type: 'KeyboardPress', press a key
  * {{ param: {{ value: string }} }}
- type: 'Scroll', scroll up or down.
  * {{ 
      ${llmLocateParam}, 
      param: {{ 
        direction: 'down'(default) | 'up' | 'right' | 'left', 
        scrollType: 'once' (default) | 'untilBottom' | 'untilTop' | 'untilRight' | 'untilLeft', 
        distance: null | number 
      }} 
    }}
    * To scroll some specific element, put the element at the center of the region in the \`locate\` field. If it's a page scroll, put \`null\` in the \`locate\` field. 
    * \`param\` is required in this action. If some fields are not specified, use direction \`down\`, \`once\` scroll type, and \`null\` distance.
- type: 'ExpectedFalsyCondition'
  * {{ param: {{ reason: string }} }}
  * use this action when the conditional statement talked about in the instruction is falsy.
- type: 'Sleep'
  * {{ param: {{ timeMs: number }} }}
`;
var outputTemplate = `
## Output JSON Format:

The JSON format is as follows:

{{
  "actions": [
    // ... some actions
  ],
  ${llmCurrentLog}
  ${commonOutputFields}
}}

## Examples

### Example: Decompose a task

When the instruction is 'Click the language switch button, wait 1s, click "English"', and not log is provided

By viewing the page screenshot and description, you should consider this and output the JSON:

* The main steps should be: tap the switch button, sleep, and tap the 'English' option
* The language switch button is shown in the screenshot, but it's not marked with a rectangle. So we have to use the page description to find the element. By carefully checking the context information (coordinates, attributes, content, etc.), you can find the element.
* The "English" option button is not shown in the screenshot now, it means it may only show after the previous actions are finished. So don't plan any action to do this.
* Log what these action do: Click the language switch button to open the language options. Wait for 1 second.
* The task cannot be accomplished (because we cannot see the "English" option now), so the \`more_actions_needed_by_instruction\` field is true.

{{
  "actions":[
    {{
      "type": "Tap", 
      "thought": "Click the language switch button to open the language options.",
      "param": null,
      "locate": {{ id: "c81c4e9a33", prompt: "The language switch button" }},
    }},
    {{
      "type": "Sleep",
      "thought": "Wait for 1 second to ensure the language options are displayed.",
      "param": {{ "timeMs": 1000 }},
    }}
  ],
  "error": null,
  "more_actions_needed_by_instruction": true,
  "log": "Click the language switch button to open the language options. Wait for 1 second",
}}

### Example: What NOT to do
Wrong output:
{{
  "actions":[
    {{
      "type": "Tap",
      "thought": "Click the language switch button to open the language options.",
      "param": null,
      "locate": {{
        {{ "id": "c81c4e9a33" }}, // WRONG: prompt is missing
      }}
    }},
    {{
      "type": "Tap", 
      "thought": "Click the English option",
      "param": null,
      "locate": null, // This means the 'English' option is not shown in the screenshot, the task cannot be accomplished
    }}
  ],
  "more_actions_needed_by_instruction": false, // WRONG: should be true
  "log": "Click the language switch button to open the language options",
}}

Reason:
* The \`prompt\` is missing in the first 'Locate' action
* Since the option button is not shown in the screenshot, there are still more actions to be done, so the \`more_actions_needed_by_instruction\` field should be true
`;
async function systemPromptToTaskPlanning() {
  if (getAIConfigInBoolean(MIDSCENE_USE_QWEN_VL)) {
    return systemTemplateOfQwen;
  }
  const promptTemplate = new PromptTemplate2({
    template: `${systemTemplateOfLLM}

${outputTemplate}`,
    inputVariables: ["pageDescription"]
  });
  return await promptTemplate.format({
    pageDescription: samplePageDescription
  });
}
var planSchema = {
  type: "json_schema",
  json_schema: {
    name: "action_items",
    strict: true,
    schema: {
      type: "object",
      strict: true,
      properties: {
        actions: {
          //  TODO
          type: "array",
          items: {
            type: "object",
            strict: true,
            properties: {
              thought: {
                type: "string",
                description: "Reasons for generating this task, and why this task is feasible on this page"
              },
              type: {
                type: "string",
                description: 'Type of action, one of "Tap", "Hover" , "Input", "KeyboardPress", "Scroll", "ExpectedFalsyCondition", "Sleep"'
              },
              param: {
                anyOf: [
                  { type: "null" },
                  {
                    type: "object",
                    properties: { value: { type: ["string", "number"] } },
                    required: ["value"],
                    additionalProperties: false
                  },
                  {
                    type: "object",
                    properties: { timeMs: { type: ["number", "string"] } },
                    required: ["timeMs"],
                    additionalProperties: false
                  },
                  {
                    type: "object",
                    properties: {
                      direction: { type: "string" },
                      scrollType: { type: "string" },
                      distance: { type: ["number", "string", "null"] }
                    },
                    required: ["direction", "scrollType", "distance"],
                    additionalProperties: false
                  },
                  {
                    type: "object",
                    properties: { reason: { type: "string" } },
                    required: ["reason"],
                    additionalProperties: false
                  }
                ],
                description: "Parameter of the action, can be null ONLY when the type field is Tap or Hover"
              },
              locate: {
                type: ["object", "null"],
                properties: {
                  id: { type: "string" },
                  prompt: { type: "string" }
                },
                required: ["id", "prompt"],
                additionalProperties: false,
                description: "Location information for the target element"
              }
            },
            required: ["thought", "type", "param", "locate"],
            additionalProperties: false
          },
          description: "List of actions to be performed"
        },
        more_actions_needed_by_instruction: {
          type: "boolean",
          description: "If all the actions described in the instruction have been covered by this action and logs, set this field to false."
        },
        log: {
          type: "string",
          description: "Log what these planned actions do. Do not include further actions that have not been planned."
        },
        error: {
          type: ["string", "null"],
          description: "Error messages about unexpected situations"
        }
      },
      required: [
        "actions",
        "more_actions_needed_by_instruction",
        "log",
        "error"
      ],
      additionalProperties: false
    }
  }
};
var generateTaskBackgroundContext = (userInstruction, log) => {
  if (log) {
    return `
Here is the user's instruction:
<instruction>
${userInstruction}
</instruction>

These are the logs from previous executions, which indicate what was done in the previous actions.
Do NOT repeat these actions.
<previous_logs>
${log}
</previous_logs>
`;
  }
  return `
Here is the user's instruction:
<instruction>
${userInstruction}
</instruction>
`;
};
var automationUserPrompt = () => {
  if (getAIConfigInBoolean(MATCH_BY_POSITION)) {
    return new PromptTemplate2({
      template: "{taskBackgroundContext}",
      inputVariables: ["taskBackgroundContext"]
    });
  }
  return new PromptTemplate2({
    template: `
pageDescription:
=====================================
{pageDescription}
=====================================

{taskBackgroundContext}
    `,
    inputVariables: ["pageDescription", "taskBackgroundContext"]
  });
};

// src/ai-model/service-caller/index.ts
function checkAIConfig() {
  if (getAIConfig(OPENAI_API_KEY))
    return true;
  if (getAIConfig(MIDSCENE_USE_AZURE_OPENAI))
    return true;
  if (getAIConfig(ANTHROPIC_API_KEY))
    return true;
  return Boolean(getAIConfig(MIDSCENE_OPENAI_INIT_CONFIG_JSON));
}
var debugProfile = getDebug("ai:profile");
var debugResponse = getDebug("ai:response");
var shouldPrintTiming = getAIConfigInBoolean(MIDSCENE_DEBUG_AI_PROFILE);
var debugConfig = "";
if (shouldPrintTiming) {
  console.warn(
    "MIDSCENE_DEBUG_AI_PROFILE is deprecated, use DEBUG=midscene:ai:profile instead"
  );
  debugConfig = "ai:profile";
}
var shouldPrintAIResponse = getAIConfigInBoolean(MIDSCENE_DEBUG_AI_RESPONSE);
if (shouldPrintAIResponse) {
  console.warn(
    "MIDSCENE_DEBUG_AI_RESPONSE is deprecated, use DEBUG=midscene:ai:response instead"
  );
  if (debugConfig) {
    debugConfig = "ai:*";
  } else {
    debugConfig = "ai:response";
  }
}
if (debugConfig) {
  enableDebug(debugConfig);
}
var defaultModel = "gpt-4o";
function getModelName() {
  let modelName = defaultModel;
  const nameInConfig = getAIConfig(MIDSCENE_MODEL_NAME);
  if (nameInConfig) {
    modelName = nameInConfig;
  }
  return modelName;
}
async function createChatClient({
  AIActionTypeValue
}) {
  let openai;
  const extraConfig = getAIConfigInJson(MIDSCENE_OPENAI_INIT_CONFIG_JSON);
  const socksProxy = getAIConfig(MIDSCENE_OPENAI_SOCKS_PROXY);
  const socksAgent = socksProxy ? new SocksProxyAgent(socksProxy) : void 0;
  if (getAIConfig(OPENAI_USE_AZURE)) {
    openai = new AzureOpenAI({
      baseURL: getAIConfig(OPENAI_BASE_URL),
      apiKey: getAIConfig(OPENAI_API_KEY),
      httpAgent: socksAgent,
      ...extraConfig,
      dangerouslyAllowBrowser: true
    });
  } else if (getAIConfig(MIDSCENE_USE_AZURE_OPENAI)) {
    const extraAzureConfig = getAIConfigInJson(
      MIDSCENE_AZURE_OPENAI_INIT_CONFIG_JSON
    );
    const scope = getAIConfig(MIDSCENE_AZURE_OPENAI_SCOPE);
    let tokenProvider = void 0;
    if (scope) {
      assert3(
        !ifInBrowser,
        "Azure OpenAI is not supported in browser with Midscene."
      );
      const credential = new DefaultAzureCredential();
      assert3(scope, "MIDSCENE_AZURE_OPENAI_SCOPE is required");
      tokenProvider = getBearerTokenProvider(credential, scope);
      openai = new AzureOpenAI({
        azureADTokenProvider: tokenProvider,
        endpoint: getAIConfig(AZURE_OPENAI_ENDPOINT),
        apiVersion: getAIConfig(AZURE_OPENAI_API_VERSION),
        deployment: getAIConfig(AZURE_OPENAI_DEPLOYMENT),
        ...extraConfig,
        ...extraAzureConfig
      });
    } else {
      openai = new AzureOpenAI({
        apiKey: getAIConfig(AZURE_OPENAI_KEY),
        endpoint: getAIConfig(AZURE_OPENAI_ENDPOINT),
        apiVersion: getAIConfig(AZURE_OPENAI_API_VERSION),
        deployment: getAIConfig(AZURE_OPENAI_DEPLOYMENT),
        dangerouslyAllowBrowser: true,
        ...extraConfig,
        ...extraAzureConfig
      });
    }
  } else if (!getAIConfig(MIDSCENE_USE_ANTHROPIC_SDK)) {
    const baseURL = getAIConfig(OPENAI_BASE_URL);
    if (typeof baseURL === "string") {
      if (!/^https?:\/\//.test(baseURL)) {
        throw new Error(
          `OPENAI_BASE_URL must be a valid URL starting with http:// or https://, but got: ${baseURL}
Please check your config.`
        );
      }
    }
    openai = new OpenAI({
      baseURL: getAIConfig(OPENAI_BASE_URL),
      apiKey: getAIConfig(OPENAI_API_KEY),
      httpAgent: socksAgent,
      ...extraConfig,
      defaultHeaders: {
        ...extraConfig?.defaultHeaders || {},
        [MIDSCENE_API_TYPE]: AIActionTypeValue.toString()
      },
      dangerouslyAllowBrowser: true
    });
  }
  if (openai && getAIConfigInBoolean(MIDSCENE_LANGSMITH_DEBUG)) {
    if (ifInBrowser) {
      throw new Error("langsmith is not supported in browser");
    }
    console.log("DEBUGGING MODE: langsmith wrapper enabled");
    const { wrapOpenAI } = await import("langsmith/wrappers");
    openai = wrapOpenAI(openai);
  }
  if (typeof openai !== "undefined") {
    return {
      completion: openai.chat.completions,
      style: "openai"
    };
  }
  if (getAIConfig(MIDSCENE_USE_ANTHROPIC_SDK)) {
    const apiKey = getAIConfig(ANTHROPIC_API_KEY);
    assert3(apiKey, "ANTHROPIC_API_KEY is required");
    openai = new Anthropic({
      apiKey,
      dangerouslyAllowBrowser: true
    });
  }
  if (typeof openai !== "undefined" && openai.messages) {
    return {
      completion: openai.messages,
      style: "anthropic"
    };
  }
  throw new Error("Openai SDK or Anthropic SDK is not initialized");
}
async function call(messages, AIActionTypeValue, responseFormat) {
  const { completion, style } = await createChatClient({
    AIActionTypeValue
  });
  const maxTokens = getAIConfig(OPENAI_MAX_TOKENS);
  const startTime = Date.now();
  const model = getModelName();
  let content;
  let usage;
  const commonConfig = {
    temperature: getAIConfigInBoolean(MIDSCENE_USE_VLM_UI_TARS) ? 0 : 0.1,
    stream: false,
    max_tokens: typeof maxTokens === "number" ? maxTokens : Number.parseInt(maxTokens || "2048", 10),
    ...getAIConfigInBoolean(MIDSCENE_USE_QWEN_VL) ? {
      vl_high_resolution_images: true
    } : {}
  };
  if (style === "openai") {
    const result = await completion.create({
      model,
      messages,
      response_format: responseFormat,
      ...commonConfig
    });
    debugProfile(
      "model %s,%s usage %s, cost %s ms, requestId %s",
      model,
      getAIConfig(MIDSCENE_USE_QWEN_VL) ? " MIDSCENE_USE_QWEN_VL," : "",
      JSON.stringify(result.usage),
      Date.now() - startTime,
      result._request_id
    );
    assert3(
      result.choices,
      `invalid response from LLM service: ${JSON.stringify(result)}`
    );
    content = result.choices[0].message.content;
    debugResponse(content);
    assert3(content, "empty content");
    usage = result.usage;
  } else if (style === "anthropic") {
    const convertImageContent = (content2) => {
      if (content2.type === "image_url") {
        const imgBase64 = content2.image_url.url;
        assert3(imgBase64, "image_url is required");
        return {
          source: {
            type: "base64",
            media_type: imgBase64.includes("data:image/png;base64,") ? "image/png" : "image/jpeg",
            data: imgBase64.split(",")[1]
          },
          type: "image"
        };
      }
      return content2;
    };
    const result = await completion.create({
      model,
      system: "You are a versatile professional in software UI automation",
      messages: messages.map((m) => ({
        role: "user",
        content: Array.isArray(m.content) ? m.content.map(convertImageContent) : m.content
      })),
      response_format: responseFormat,
      ...commonConfig
    });
    content = result.content[0].text;
    assert3(content, "empty content");
    usage = result.usage;
  }
  return { content: content || "", usage };
}
async function callToGetJSONObject(messages, AIActionTypeValue) {
  let responseFormat;
  const model = getModelName();
  if (model.includes("gpt-4o")) {
    switch (AIActionTypeValue) {
      case 0 /* ASSERT */:
        responseFormat = assertSchema;
        break;
      case 1 /* INSPECT_ELEMENT */:
        responseFormat = locatorSchema;
        break;
      case 2 /* EXTRACT_DATA */:
        responseFormat = { type: "json_object" /* JSON */ };
        break;
      case 3 /* PLAN */:
        responseFormat = planSchema;
        break;
    }
  }
  if (model === "gpt-4o-2024-05-13") {
    responseFormat = { type: "json_object" /* JSON */ };
  }
  const response = await call(messages, AIActionTypeValue, responseFormat);
  assert3(response, "empty response");
  const jsonContent = safeParseJson(response.content);
  return { content: jsonContent, usage: response.usage };
}
function extractJSONFromCodeBlock(response) {
  try {
    const jsonMatch = response.match(/^\s*(\{[\s\S]*\})\s*$/);
    if (jsonMatch) {
      return jsonMatch[1];
    }
    const codeBlockMatch = response.match(
      /```(?:json)?\s*(\{[\s\S]*?\})\s*```/
    );
    if (codeBlockMatch) {
      return codeBlockMatch[1];
    }
    const jsonLikeMatch = response.match(/\{[\s\S]*\}/);
    if (jsonLikeMatch) {
      return jsonLikeMatch[0];
    }
  } catch {
  }
  return response;
}
function safeParseJson(input) {
  const cleanJsonString = extractJSONFromCodeBlock(input);
  if (cleanJsonString?.match(/\((\d+),(\d+)\)/)) {
    return cleanJsonString.match(/\((\d+),(\d+)\)/)?.slice(1).map(Number);
  }
  try {
    return JSON.parse(cleanJsonString);
  } catch {
  }
  try {
    return dJSON.parse(cleanJsonString);
  } catch (e) {
    console.log("e:", e);
  }
  throw Error(`failed to parse json response: ${input}`);
}

// src/ai-model/inspect.ts
import { paddingToMatchBlock } from "@midscene/shared/img";
import { assert as assert4 } from "@midscene/shared/utils";

// src/ai-model/prompt/extraction.ts
import { PromptTemplate as PromptTemplate3 } from "@langchain/core/prompts";
function systemPromptToExtract() {
  return `
You are a versatile professional in software UI design and testing. Your outstanding contributions will impact the user experience of billions of users.

The user will give you a screenshot, the contents of it (optional), and some data requirements in DATA_DEMAND. You need to extract the data according to the DATA_DEMAND.

Return in the following JSON format:
{
  data: any, // the extracted data from extract_data_from_UI skill. Make sure both the value and scheme meet the DATA_DEMAND. If you want to write some description in this field, use the same language as the DATA_DEMAND.
  errors: [], // string[], error message if any
}
`;
}
var extractDataPrompt = new PromptTemplate3({
  template: `
pageDescription: {pageDescription}

Use your extract_data_from_UI skill to find the following data, placing it in the \`data\` field
DATA_DEMAND start:
=====================================
{dataKeys}

{dataQuery}
=====================================
DATA_DEMAND ends.
  `,
  inputVariables: ["pageDescription", "dataKeys", "dataQuery"]
});

// src/ai-model/inspect.ts
var liteContextConfig = {
  filterNonTextContent: true,
  truncateTextLength: 200
};
function transformToAbsoluteCoords(relativePosition, size) {
  return {
    x: Number((relativePosition.x / 1e3 * size.width).toFixed(3)),
    y: Number((relativePosition.y / 1e3 * size.height).toFixed(3))
  };
}
async function transformElementPositionToId(aiResult, treeRoot, size, insertElementByPosition) {
  const emptyResponse = {
    errors: [],
    elements: []
  };
  const elementAtPosition = (center) => {
    const element = elementByPositionWithElementInfo(treeRoot, center);
    const distanceToCenter = element ? distance({ x: element.center[0], y: element.center[1] }, center) : 0;
    return distanceToCenter <= distanceThreshold ? element : void 0;
  };
  if ("bbox" in aiResult) {
    if (!Array.isArray(aiResult.bbox) || aiResult.bbox.length !== 4) {
      return emptyResponse;
    }
    aiResult.bbox[0] = Math.ceil(aiResult.bbox[0]);
    aiResult.bbox[1] = Math.ceil(aiResult.bbox[1]);
    aiResult.bbox[2] = Math.ceil(aiResult.bbox[2]);
    aiResult.bbox[3] = Math.ceil(aiResult.bbox[3]);
    const centerX = (aiResult.bbox[0] + aiResult.bbox[2]) / 2;
    const centerY = (aiResult.bbox[1] + aiResult.bbox[3]) / 2;
    let element = elementAtPosition({ x: centerX, y: centerY });
    if (!element) {
      element = insertElementByPosition({
        x: centerX,
        y: centerY
      });
    }
    assert4(
      element,
      `inspect: no element found with coordinates: ${JSON.stringify(aiResult.bbox)}`
    );
    return {
      errors: [],
      elements: [
        {
          id: element.id
        }
      ]
    };
  }
  if (Array.isArray(aiResult)) {
    const relativePosition = aiResult;
    const absolutePosition = transformToAbsoluteCoords(
      {
        x: relativePosition[0],
        y: relativePosition[1]
      },
      size
    );
    let element = elementAtPosition(absolutePosition);
    if (!element) {
      element = insertElementByPosition(absolutePosition);
    }
    assert4(
      element,
      `inspect: no id found with position: ${JSON.stringify({ absolutePosition })}`
    );
    return {
      errors: [],
      elements: [
        {
          id: element.id
        }
      ]
    };
  }
  return {
    errors: aiResult.errors,
    elements: aiResult.elements
  };
}
function matchQuickAnswer(quickAnswer, tree, elementById, insertElementByPosition) {
  if (!quickAnswer) {
    return void 0;
  }
  if ("id" in quickAnswer && quickAnswer.id && elementById(quickAnswer.id)) {
    return {
      parseResult: {
        elements: [quickAnswer],
        errors: []
      },
      rawResponse: quickAnswer,
      elementById
    };
  }
  if ("position" in quickAnswer && quickAnswer.position) {
    let element = elementByPositionWithElementInfo(tree, quickAnswer.position);
    if (!element) {
      element = insertElementByPosition(quickAnswer.position);
    }
    return {
      parseResult: {
        elements: [element],
        errors: []
      },
      rawResponse: quickAnswer,
      elementById
    };
  }
  if ("bbox" in quickAnswer && quickAnswer.bbox) {
    const centerPosition = {
      x: Math.floor((quickAnswer.bbox[0] + quickAnswer.bbox[2]) / 2),
      y: Math.floor((quickAnswer.bbox[1] + quickAnswer.bbox[3]) / 2)
    };
    let element = elementByPositionWithElementInfo(tree, centerPosition);
    if (!element) {
      element = insertElementByPosition(centerPosition);
    }
    return {
      parseResult: {
        elements: [element],
        errors: []
      },
      rawResponse: quickAnswer,
      elementById
    };
  }
  return void 0;
}
async function AiInspectElement(options) {
  const { context, targetElementDescription, callAI } = options;
  const { screenshotBase64, screenshotBase64WithElementMarker } = context;
  const { description, elementById, insertElementByPosition, size } = await describeUserPage(context);
  const quickAnswer = matchQuickAnswer(
    options.quickAnswer,
    context.tree,
    elementById,
    insertElementByPosition
  );
  if (quickAnswer) {
    return quickAnswer;
  }
  assert4(
    targetElementDescription,
    "cannot find the target element description"
  );
  const userInstructionPrompt = await findElementPrompt.format({
    pageDescription: description,
    targetElementDescription
  });
  const systemPrompt = systemPromptToLocateElement();
  let imagePayload = screenshotBase64WithElementMarker || screenshotBase64;
  if (getAIConfigInBoolean(MIDSCENE_USE_QWEN_VL)) {
    imagePayload = await paddingToMatchBlock(imagePayload);
  }
  const msgs = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: [
        {
          type: "image_url",
          image_url: {
            url: imagePayload,
            detail: "high"
          }
        },
        {
          type: "text",
          text: userInstructionPrompt
        }
      ]
    }
  ];
  const callAIFn = callAI || callToGetJSONObject;
  const res = await callAIFn(msgs, 1 /* INSPECT_ELEMENT */);
  const parseResult = await transformElementPositionToId(
    res.content,
    context.tree,
    size,
    insertElementByPosition
  );
  return {
    parseResult,
    rawResponse: res.content,
    elementById,
    usage: res.usage
  };
}
async function AiExtractElementInfo(options) {
  const { dataQuery, context } = options;
  const systemPrompt = systemPromptToExtract();
  const { screenshotBase64 } = context;
  const { description, elementById } = await describeUserPage(
    context,
    liteContextConfig
  );
  let dataKeys = "";
  let dataQueryText = "";
  if (typeof dataQuery === "string") {
    dataKeys = "";
    dataQueryText = dataQuery;
  } else {
    dataKeys = `return in key-value style object, keys are ${Object.keys(dataQuery).join(",")}`;
    dataQueryText = JSON.stringify(dataQuery, null, 2);
  }
  const extractDataPromptText = await extractDataPrompt.format({
    pageDescription: description,
    dataKeys,
    dataQuery: dataQueryText
  });
  const msgs = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: [
        {
          type: "image_url",
          image_url: {
            url: screenshotBase64,
            detail: "high"
          }
        },
        {
          type: "text",
          text: extractDataPromptText
        }
      ]
    }
  ];
  const result = await callAiFn(
    msgs,
    2 /* EXTRACT_DATA */
  );
  return {
    parseResult: result.content,
    elementById,
    usage: result.usage
  };
}
async function AiAssert(options) {
  const { assertion, context } = options;
  assert4(assertion, "assertion should be a string");
  const { screenshotBase64 } = context;
  const systemPrompt = systemPromptToAssert({
    isUITars: getAIConfigInBoolean(MIDSCENE_USE_VLM_UI_TARS)
  });
  const msgs = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: [
        {
          type: "image_url",
          image_url: {
            url: screenshotBase64,
            detail: "high"
          }
        },
        {
          type: "text",
          text: `
Here is the assertion. Please tell whether it is truthy according to the screenshot.
=====================================
${assertion}
=====================================
  `
        }
      ]
    }
  ];
  const { content: assertResult, usage } = await callAiFn(
    msgs,
    0 /* ASSERT */
  );
  return {
    content: assertResult,
    usage
  };
}

// src/ai-model/llm-planning.ts
import { paddingToMatchBlock as paddingToMatchBlock2 } from "@midscene/shared/img";
import { assert as assert5 } from "@midscene/shared/utils";
async function plan(userInstruction, opts) {
  const { callAI, context } = opts || {};
  const { screenshotBase64, screenshotBase64WithElementMarker, size } = context;
  const { description: pageDescription } = await describeUserPage(context);
  const systemPrompt = await systemPromptToTaskPlanning();
  const taskBackgroundContextText = generateTaskBackgroundContext(
    userInstruction,
    opts.log
  );
  const userInstructionPrompt = await automationUserPrompt().format({
    pageDescription,
    taskBackgroundContext: taskBackgroundContextText
  });
  let imagePayload = screenshotBase64WithElementMarker || screenshotBase64;
  if (getAIConfigInBoolean(MIDSCENE_USE_QWEN_VL)) {
    imagePayload = await paddingToMatchBlock2(imagePayload);
  }
  warnGPT4oSizeLimit(size);
  const msgs = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: [
        {
          type: "image_url",
          image_url: {
            url: imagePayload,
            detail: "high"
          }
        },
        {
          type: "text",
          text: userInstructionPrompt
        }
      ]
    }
  ];
  const call2 = callAI || callAiFn;
  const { content, usage } = await call2(msgs, 3 /* PLAN */);
  const rawResponse = JSON.stringify(content, void 0, 2);
  const planFromAI = content;
  const actions = (planFromAI.action?.type ? [planFromAI.action] : planFromAI.actions) || [];
  const returnValue = {
    ...planFromAI,
    actions,
    rawResponse,
    usage
  };
  assert5(planFromAI, "can't get plans from AI");
  if (getAIConfigInBoolean(MIDSCENE_USE_QWEN_VL)) {
    actions.forEach((action) => {
      if (action.locate) {
        action.locate = fillLocateParam(action.locate);
      }
    });
    assert5(!planFromAI.error, `Failed to plan actions: ${planFromAI.error}`);
  }
  if (actions.length === 0 && returnValue.more_actions_needed_by_instruction && !returnValue.sleep) {
    console.warn(
      "No actions planned for the prompt, but model said more actions are needed:",
      userInstruction
    );
  }
  return returnValue;
}

// src/ai-model/ui-tars-planning.ts
import { transformHotkeyInput } from "@midscene/shared/keyboard-layout";
import { assert as assert6 } from "@midscene/shared/utils";
import { actionParser } from "@ui-tars/action-parser";
async function vlmPlanning(options) {
  const { conversationHistory, userInstruction, size } = options;
  const systemPrompt = uiTarsPlanningPrompt + userInstruction;
  const res = await call(
    [
      {
        role: "user",
        content: systemPrompt
      },
      ...conversationHistory
    ],
    1 /* INSPECT_ELEMENT */
  );
  const { parsed } = actionParser({
    prediction: res.content,
    factor: 1e3
  });
  const transformActions = [];
  parsed.forEach((action) => {
    if (action.action_type === "click") {
      assert6(action.action_inputs.start_box, "start_box is required");
      const point = getPoint(action.action_inputs.start_box, size);
      transformActions.push({
        type: "Locate",
        locate: {
          prompt: action.thought || "",
          position: { x: point[0], y: point[1] }
        },
        param: {
          // action,
          // position: { x: point[0], y: point[1] },
        }
      });
      transformActions.push({
        type: "Tap",
        locate: {
          prompt: action.thought || "",
          position: { x: point[0], y: point[1] }
        },
        param: action.thought || ""
      });
    } else if (action.action_type === "drag") {
      assert6(action.action_inputs.start_box, "start_box is required");
      assert6(action.action_inputs.end_box, "end_box is required");
      const startPoint = getPoint(action.action_inputs.start_box, size);
      const endPoint = getPoint(action.action_inputs.end_box, size);
      transformActions.push({
        type: "Drag",
        param: {
          start_box: { x: startPoint[0], y: startPoint[1] },
          end_box: { x: endPoint[0], y: endPoint[1] }
        },
        locate: null,
        thought: action.thought || ""
      });
    } else if (action.action_type === "type") {
      transformActions.push({
        type: "Input",
        param: {
          value: action.action_inputs.content
        },
        locate: null,
        thought: action.thought || ""
      });
    } else if (action.action_type === "scroll") {
      transformActions.push({
        type: "Scroll",
        param: {
          direction: action.action_inputs.direction
        },
        locate: null,
        thought: action.thought || ""
      });
    } else if (action.action_type === "finished") {
      transformActions.push({
        type: "Finished",
        param: {},
        locate: null,
        thought: action.thought || ""
      });
    } else if (action.action_type === "hotkey") {
      assert6(action.action_inputs.key, "key is required");
      const keys = transformHotkeyInput(action.action_inputs.key);
      transformActions.push({
        type: "KeyboardPress",
        param: {
          value: keys
        },
        locate: null,
        thought: action.thought || ""
      });
    } else if (action.action_type === "wait") {
      transformActions.push({
        type: "Sleep",
        param: {
          timeMs: 1e3
        },
        locate: null,
        thought: action.thought || ""
      });
    }
  });
  if (transformActions.length === 0) {
    throw new Error(`No actions found, response: ${res.content}`, {
      cause: {
        prediction: res.content,
        parsed
      }
    });
  }
  return {
    actions: transformActions,
    realActions: parsed,
    action_summary: getSummary(res.content)
  };
}
function getPoint(startBox, size) {
  const [x, y] = JSON.parse(startBox);
  return [x * size.width, y * size.height];
}

export {
  systemPromptToLocateElement,
  describeUserPage,
  callToGetJSONObject,
  callAiFn,
  transformElementPositionToId,
  AiInspectElement,
  AiExtractElementInfo,
  AiAssert,
  plan,
  vlmPlanning
};

//# sourceMappingURL=chunk-HHN545QP.js.map